{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A7 Task",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOLL1cHE3O0YOzrDZnw3aRq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poudel-bibek/Intro-to-AI-Assignments/blob/main/A7_Task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Artboard](https://user-images.githubusercontent.com/96804013/153070586-1d2aba89-08a4-46ae-b570-807ed3e34201.png)\n"
      ],
      "metadata": {
        "id": "Ej_B1a1WLIuC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 7: AutoEncoder and Variational AutoEncoder (Task)\n",
        "---\n",
        "\n",
        "In this assignment, we will train an AutoEncoder that can denoise images, making use the CIFAR10 dataset ([link](https://en.wikipedia.org/wiki/CIFAR-10)).\n",
        "\n",
        "In addition, we will train a Variational AutoEncoder that can learn a distribution of low dimensional representation on the MNIST dataset ([link](https://en.wikipedia.org/wiki/MNIST_database)) and sample new, \"fake\" images when a distribution is learned.  \n",
        "\n",
        "## 7-1. AutoEncoder\n",
        "\n",
        "Let's start by importing the necessary packages.\n",
        "\n",
        "                    import random\n",
        "                    import numpy as np\n",
        "                    import matplotlib.pyplot as plt\n",
        "\n",
        "                    import tensorflow as tf \n",
        "                    from tensorflow import keras \n",
        "                    from keras.datasets import mnist, cifar10\n",
        "                    from keras.models import Sequential\n",
        "\n",
        "                    from keras import Model\n",
        "                    from keras.layers import Conv2D, MaxPooling2D, Conv2DTranspose, Dense, Flatten, Reshape, Input\n",
        "\n",
        "\n",
        "Next, let's set some seeds for experiment reproducibility.\n",
        "\n",
        "                    SEED = 99\n",
        "                    random.seed(SEED)\n",
        "                    np.random.seed(SEED)\n",
        "                    tf.random.set_seed(SEED)"
      ],
      "metadata": {
        "id": "-9gQm4meLIsF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Exercise 1\n",
        "- Load the CIFAR 10 dataset, store values in `x_train`, `y_train`, `x_test`, `y_test` respectively. \n",
        "- Randomly sample a subset of `x_train` and `x_test` sets such that the shapes are the following.\n",
        "  - x_train.shape = (15000, 32, 32, 3) \n",
        "  - x_test.shape = (1000, 32, 32, 3) \n",
        "  - Reference ([link](https://docs.python.org/3/library/random.html#random.sample))\n"
      ],
      "metadata": {
        "id": "E67lmVnjZd_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's normalize `x_train` and `x_test` and set input shape using the following code: \n",
        "\n",
        "                    x_train = x_train.astype('float32')/ 255.\n",
        "                    x_test = x_test.astype('float32')/ 255.\n",
        "\n",
        "                    input_shape = (x_train.shape[1], x_train.shape[2], 3)"
      ],
      "metadata": {
        "id": "7wWUDOpcZgjg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Exercise 2\n",
        "\n",
        "- Fill in the line with your code below using numpy.random.sample to sample a gaussian noise of `mean` and `std` specified. \n",
        "- Make sure the size of the noise is same as the size of `input_data`  \n",
        "- Reference ([link](https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html))\n",
        "\n",
        "                    def add_noise(input_data):\n",
        "                      mean , std = 0.0, 0.15 \n",
        "                      noise = ################# YOUR CODE HERE ##################\n",
        "                      noisy_data = np.clip(input_data + noise, 0., 1.) \n",
        "                      return noisy_data"
      ],
      "metadata": {
        "id": "ogso2yMDZioY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, call the function you just wrote and plot noisy and clean images side by side. \n",
        "\n",
        "                    noisy_x_train = add_noise(x_train)\n",
        "                    noisy_x_test = add_noise(x_test)\n",
        "\n",
        "                    fig, ax = plt.subplots(4,4,figsize = (12,12), dpi = 80)\n",
        "                    for i, ax in enumerate(fig.axes):\n",
        "                      if i%2 ==0 :  \n",
        "                        ax.imshow(np.squeeze(x_train[i]))\n",
        "                        ax.set_title(f\"Clean\")\n",
        "                      else: \n",
        "                        ax.imshow(np.squeeze(noisy_x_train[i-1]))\n",
        "                        ax.set_title(\"Noisy\")\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://user-images.githubusercontent.com/96804013/153232811-6167c633-171c-4093-b31e-c2ec78a39338.png\")\n",
        "\"/>\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <em>Figure 1: Noise addded to clean images in the CIFAR10 dataset</em>\n",
        "</p>\n",
        "\n"
      ],
      "metadata": {
        "id": "_aVB4nZfZlKk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The architecture of an AutoEncoder consists of 2 parts, the Encoder and the Decoder. \n",
        "\n",
        "We will be coding both parts in a single model called `autoencoder`.\n",
        "\n",
        "---\n",
        "## Exercise 3\n",
        "\n",
        "Initialize a Sequential model with following code: \n",
        "\n",
        "                  autoencoder = Sequential()\n",
        "\n",
        "- Use `model.add()` function to build an AutoEncoder model with the following architecture.\n",
        "\n",
        "  - 1st layer: convolutional layer with `28` filters, kernel size `3` by `3`, activation function ReLU.\n",
        "  - 2nd layer: pooling layer with max pooling and pooling size `2` by `2`.\n",
        "  - 3rd layer: convolutional layer with `28` filters, kernel size `3` by `3`, activation function ReLU.\n",
        "  - 4th layer: pooling layer with max pooling and pooling size `2` by `2`.\n",
        "  This completes the encoder part\n",
        "  - 5th layer: inverse convolution (Conv2DTranspose) layer with `28` filters, kernel size `3` by `3`, stride of `2` and activation function ReLU\n",
        "  - 6th layer: inverse convolution (Conv2DTranspose) layer with `28` filters, kernel size `3` by `3`, stride of `2` and activation function ReLU\n",
        "  - 7th layer: convolutional layer with `3` filters, kernel size `3` by `3`, activation function sigmoid.\n",
        "\n",
        "- Use `autoencoder.summary()` to check the built CNN encoder.\n",
        "\n",
        "- References: \n",
        "  - Conv2D ([link](https://keras.io/api/layers/convolution_layers/convolution2d/)) \n",
        "  - MaxPool ([link](https://keras.io/api/layers/pooling_layers/max_pooling2d/))\n",
        "  - Conv2DTranspose ([link](https://keras.io/api/layers/convolution_layers/convolution2d_transpose/))\n"
      ],
      "metadata": {
        "id": "YogDmjEqZn7I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, compile and fit the autoencoder using: \n",
        "\n",
        "                    autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "                    autoencoder.fit(noisy_x_train, x_train,\n",
        "                                    epochs = 20,\n",
        "                                    batch_size = 256, \n",
        "                                    shuffle = True,\n",
        "                                    verbose =2)\n",
        "\n",
        "Notice that the input data for training is `x_train`, `x_train`. Why? \n",
        "- Expect a training time of ~11 minutes"
      ],
      "metadata": {
        "id": "irCxyYyWZqjH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the model for future use: \n",
        "\n",
        "                    autoencoder.save(\"my_denoiser\")\n",
        "\n",
        "Load the saved model and plot noise/ denoised `test` set images \n",
        "\n",
        "                    loaded_model = keras.models.load_model(\"my_denoiser\")\n",
        "                    denoised_x_test= loaded_model.predict(x_test)\n",
        "\n",
        "                    fig, ax = plt.subplots(6,3,figsize = (8,19), dpi = 80)\n",
        "                    for i, ax in enumerate(fig.axes):\n",
        "                      if i%3 ==0 :  \n",
        "                        ax.imshow(np.squeeze(x_test[i]))\n",
        "                        ax.set_title(f\"Clean\")\n",
        "                      elif i%3==1:\n",
        "                        ax.imshow(np.squeeze(noisy_x_test[i-1]))\n",
        "                        ax.set_title(\"Noisy\")\n",
        "                      else: \n",
        "                        ax.imshow(np.squeeze(denoised_x_test[i-2]))\n",
        "                        ax.set_title(\"AE Denoised\")\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://user-images.githubusercontent.com/96804013/154810843-58481cea-10b9-4f20-bc71-f78332d16e14.png\")\n",
        "\"/>\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <em>Figure 2: Clean, Noisy and Denoised images</em>\n",
        "</p>\n"
      ],
      "metadata": {
        "id": "Y-Hu-a66Zta9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 7-2. Variational AutoEncoder\n",
        "\n",
        "- Instead of directly learning the latent features from input samples, a VAE learns the distribution of latent features. \n",
        "- This gives us the ability to sample \"new\" images from a learned distribution (a.k.a. VAE can be used as a generative model) \n",
        "- We will be reducing the images from MNIST dataset to a vector of just 2 dimensions (x, y). \n",
        "- Then, we can take a random vector for e.g., (2.5, 0.5) and it will correspond to an image.   \n",
        "\n",
        "---\n",
        "## Exercise 1: \n",
        "- load `mnist` data to `x_train`, `x_test`, `y_train`, `y_test`\n",
        "- reshape `x_train` to a shape (`x_train.shape[0]`, `28`, `28`, `1`) \n",
        "- change type of x_train to `float32` and Normalize pixels to have a range `[0, 1]`"
      ],
      "metadata": {
        "id": "RswEsjpRZxzb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Unlike AutoEncoder, in VAE, we will create disjoint Encoder, Decoder, Sampling layers and stitch them together.  \n",
        "\n",
        "- Use the following code to set input shape, set latent dimensions and implement a custom sampling layer. \n",
        "\n",
        "                input_shape = (x_train.shape[1], x_train.shape[2], 1)\n",
        "                latent_dims = 2\n",
        "\n",
        "                class Sampling(keras.layers.Layer):\n",
        "                  def call(self, distribution):\n",
        "                    mean, log_variance = distribution \n",
        "                    sample = tf.keras.backend.random_normal(shape = (tf.shape(mean)[0], tf.shape(mean)[1] ) )\n",
        "                    return mean + tf.exp(0.5*log_variance)*sample  # mean + std"
      ],
      "metadata": {
        "id": "KsSZjfwCZ0he"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the following code to build an encoder:\n",
        "\n",
        "                    encoder_input = Input(shape = input_shape)\n",
        "\n",
        "                    x = Conv2D(32, (3, 3), activation = \"relu\", strides=2)(encoder_input)\n",
        "                    x = Conv2D(64, (3, 3), activation = \"relu\", strides = 2)(x)\n",
        "                    x = Flatten()(x)\n",
        "                    x = Dense(16, activation = \"relu\")(x)\n",
        "\n",
        "                    mean = Dense(latent_dims)(x)\n",
        "                    variance = Dense(latent_dims)(x)\n",
        "                    encoded_input = Sampling()([mean, variance])\n",
        "\n",
        "                    encoder = Model(encoder_input, [mean, variance, encoded_input], name = \"encoder\")\n",
        "                    encoder.summary()"
      ],
      "metadata": {
        "id": "uvfgiZ9MaDZ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the following code to build a decoder: \n",
        "\n",
        "                    decoder_input = keras.Input(shape = (latent_dims,))\n",
        "\n",
        "                    x = Dense(7*7*64, activation = \"relu\")(decoder_input)\n",
        "                    x =  Reshape((7,7,64))(x)\n",
        "                    x = Conv2DTranspose(64, (3,3), activation = \"relu\", strides = 2, padding=\"same\")(x)\n",
        "                    x = Conv2DTranspose(32, (3,3), activation = \"relu\", strides = 2, padding = \"same\")(x)\n",
        "                    reconstructed_img = Conv2DTranspose(1, (3,3),  activation = \"sigmoid\", padding = \"same\")(x)\n",
        "\n",
        "                    decoder = keras.Model(decoder_input, reconstructed_img, name = \"decoder\")\n",
        "                    decoder.summary()"
      ],
      "metadata": {
        "id": "IPsEXQu-aF5q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The loss function for VAE is a combination of image reconstruction loss as well as KL-Divergence term. To skip coding it here, we will import some code and make use of it. Run the cell below to import code and stitch encoder and decoder together."
      ],
      "metadata": {
        "id": "ojQ6RN3faI0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get some hosted files\n",
        "!wget -o -q https://github.com/poudel-bibek/Intro-to-AI-Assignments/files/8026719/custom_vae.zip\n",
        "!unzip -o -q ./custom_vae -d ./custom_modules/\n",
        "\n",
        "# Contains a custom loss function implementation and stitches encoder and decoder\n",
        "from custom_modules.custom_vae import VAE\n",
        "\n",
        "vae = VAE(encoder, decoder)"
      ],
      "metadata": {
        "id": "jd-Ar6m9ZeR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Exercise 2\n",
        "\n",
        "Fill in the function below with your code to do the following: \n",
        "1. Compile `vae` with `adam` optimizer \n",
        "2. Fit `vae` to `x_train` (NOT `x_train`, `x_train`) for `30` epochs, batch size of `128` and shuffle set to `True`.\n",
        "\n",
        "                    def training(vae, train_now):\n",
        "\n",
        "                      if train_now:\n",
        "                        ############ YOUR CODE HERE (1) ############\n",
        "                        ############ YOUR CODE HERE (2) ############\n",
        "\n",
        "                        vae.decoder.save(\"my_decoder\")\n",
        "                        trained_vae_decoder = tf.keras.models.load_model(\"my_decoder\")\n",
        "                      else: \n",
        "                        # Get the hosted trained model\n",
        "                        !wget -o -q https://github.com/poudel-bibek/Intro-to-AI-Assignments/files/8102846/my_trained_vae_decoder.zip\n",
        "                        !unzip -o -q ./my_trained_vae_decoder.zip -d unzipped/ \n",
        "                        trained_vae_decoder = tf.keras.models.load_model('./unzipped/my_trained_vae_decoder')\n",
        "\n",
        "                      return trained_vae_decoder\n",
        "\n",
        "- Reference: Compile and fit ([link](https://keras.io/api/models/model_training_apis/))\n"
      ],
      "metadata": {
        "id": "PwOCy7UqaNLn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, execute the cell  below"
      ],
      "metadata": {
        "id": "r8pNOJaBaeTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trained_vae_decoder = training(vae, True)"
      ],
      "metadata": {
        "id": "QZEzV7ctaKuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the expected training time of this model is ~35 minutes, \n",
        "in class, when the training starts, terminate the cell above and run the cell below."
      ],
      "metadata": {
        "id": "zXw00Tb0ahUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trained_vae_decoder = training(vae, False)"
      ],
      "metadata": {
        "id": "Gawgrw3mahAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the following code use decoder of trained vae to sample data from the learned distribution and plot it.\n",
        "\n",
        "                    fig, ax = plt.subplots(4,4,figsize = (12,13), dpi = 80)\n",
        "                    for i, ax in enumerate(fig.axes):\n",
        "                      random_latent_vector = np.random.rand(latent_dims)\n",
        "                      fake_image = trained_vae_decoder.predict(random_latent_vector.reshape(1,-1))[0]\n",
        "                      ax.imshow(fake_image.reshape(28, 28), cmap= 'gray') \n",
        "                      ax.set_title(f\"vector = {round(random_latent_vector[0],2), round(random_latent_vector[1],2)}\")\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://user-images.githubusercontent.com/96804013/153247392-0b43da98-66fa-474d-86c9-3797b4e7d332.png\")\n",
        "\"/>\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <em>Figure 3: Sample images from a learned distribution using random latent vectors (the vectors and sampled numbers may be different in your case)</em>\n",
        "</p>\n",
        "\n"
      ],
      "metadata": {
        "id": "oz1EibF3ag1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OTFPbo8SayLC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}