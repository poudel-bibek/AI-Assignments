{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A2 Task.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poudel-bibek/Intro-to-AI-Assignments/blob/main/A2_Task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![h_2](https://user-images.githubusercontent.com/96804013/151669179-182425bd-4969-4452-9846-e274e5b92304.png)"
      ],
      "metadata": {
        "id": "S8vFgMBhsyhx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 2: Linear Models (Solution)\n",
        "---\n",
        "\n",
        "## 2-1. Linear Classification\n",
        "\n",
        "Please complete the following steps. \n",
        "\n",
        "- Load the [diabetes dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html): set `return_X_y` to `True`.\n",
        "\n",
        "\n",
        "- Perform the 7:2:1 split of the dataset. Please store the training set in `X_train` and `y_train`.\n",
        "\n",
        "\n",
        "- Train a linear classifier on the dataset using the built-in functions provided by `sklearn` as follows.\n",
        "\n",
        "\n",
        "                    from sklearn.linear_model import SGDClassifier\n",
        "                    clf = SGDClassifier(random_state=42)\n",
        "                    clf.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "- `X_train` and `y_train` are training sets. The coefficients $\\theta$ and intercept $\\theta_0$ can be accessed as `clf.coef_` and `clf.intercept_`, respectively.\n",
        "\n",
        "\n",
        "- Check the dimensions of `clf.coef_` and `clf.intercept_`.\n",
        "\n",
        "\n",
        "- Use the computed coefficients and intercept to predict the labels of the training set, validation set, and test set.\n",
        "\n",
        "\n",
        "- Convert those labels from numerical to categorical using the average of `diabetes_y`.\n",
        "\n",
        "\n",
        "- Verify the converted binary labels (train, validation, and test) using the built-in function `clf.predict()`. Are the results the same?\n",
        "\n",
        "\n",
        "- Compute the training error, validation error, and test error (or equivalently, training accuracy, validation accuracy, and test accuracy)."
      ],
      "metadata": {
        "id": "gz9S0Y0kxlTN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 2-2. Linear Regression\n",
        "We are going to write a gradient-based method to solve a linear regression task. First, you can use the following code to generate and inspect the data.\n",
        "\n",
        "\n",
        "                    from sklearn.datasets import make_regression\n",
        "                    import matplotlib.pyplot as plt\n",
        "                    X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
        "                    fig, ax = plt.subplots(figsize = (10,6), dpi = 100)\n",
        "                    ax.scatter(X,y,label=\"training data\")\n",
        "                    ax.legend()\n",
        "\n",
        "\n",
        "Next, we are going to implement two functions for running the following code.\n",
        "\n",
        "\n",
        "                    ### Initial parameters\n",
        "                    learning_rate = 0.01\n",
        "                    init_slope = 0\n",
        "                    init_intercept = 0\n",
        "                    num_iter = 1000\n",
        "                    [slope, intercept] = gradient_descent(X, y, init_slope, init_intercept, learning_rate, num_iter)\n",
        "\n",
        "The pseudocode for the gradient descent function is provided below.\n",
        "\n",
        "![pseudo](https://user-images.githubusercontent.com/96804013/152409472-fe7a981f-eec9-471e-b684-bc028dd4c872.png)\n",
        "\n",
        "\n",
        "\n",
        "The signatures of the two functions are provided below.\n",
        "\n",
        "\n",
        "                    ### the main gradient descent function\n",
        "                    def gradient_descent(X, y, init_slope, init_intercept, learning_rate, num_iter)\n",
        "\n",
        "                    ### function for taking one gradient step\n",
        "                    def gradient_one_step(X, y, slope, intercept, learning_rate):\n",
        "\n",
        "Finally, using the produced slope and intercept, you can visualize the fitted line using the following code. Try different iteration numbers and learning rates to see the changes.\n",
        "\n",
        "\n",
        "                    fig, ax = plt.subplots(figsize=(10,6), dpi = 80)\n",
        "                    ax.scatter(X,y,label=\"training data\") # Plot the generated data again\n",
        "                    ax.plot(X, slope*X + intercept, color=\"red\", label=\"fitted line\") # Plot the fitted line on top f the generated data\n",
        "                    ax.set_xlabel('X')\n",
        "                    ax.set_ylabel('y')\n",
        "                    ax.legend()"
      ],
      "metadata": {
        "id": "wEhQ-cevyUbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zNC0X3Rbzz9p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}