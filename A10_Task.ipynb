{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A10 Task.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM4rqHv1aDbt75MlpQqDPua",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poudel-bibek/Intro-to-AI-Assignments/blob/main/A10_Task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![bhi](https://user-images.githubusercontent.com/96804013/152689231-de4db6bd-e653-4dfc-8881-dc3feef3389d.png)\n"
      ],
      "metadata": {
        "id": "7uw5Bk7VO1bU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 10: Deep Q-Networks (Task)\n",
        "---\n",
        "\n",
        "In this assignment, we will use OpenAI Gym ([link](https://gym.openai.com/envs/CartPole-v1/)) to solve a classic control problem of Balancing a Cartpole. To do so, we will be training a Reinforcement Learning agent using the DQN algorithm ([Link](https://openai.com/blog/openai-baselines-dqn/)).\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://user-images.githubusercontent.com/96804013/152690560-12ee45f2-69bb-422c-b230-90e4430f15b3.gif\")\n",
        "\"/>\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <em>Figure 1: Performance of an untrained agent on Cartpole-v1</em>\n",
        "</p>\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://user-images.githubusercontent.com/96804013/152689694-f72560fd-cbc1-4f55-87fc-c5e21655d667.gif\")\n",
        "\"/>\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <em>Figure 2: Performance of agent during training</em>\n",
        "</p>\n",
        "\n",
        "\n",
        "Let's start by importing necessary packages, libraries and specifying the environment. \n",
        "\n",
        "                    import time\n",
        "                    import gym\n",
        "                    import random\n",
        "                    import numpy as np\n",
        "                    from collections import deque\n",
        "\n",
        "                    import tensorflow as tf\n",
        "                    from tensorflow import keras\n",
        "                    from keras.models import Sequential\n",
        "                    from keras.layers import Dense\n",
        "\n",
        "                    env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "After that, please set the randomization seeds for experiment reproducibility. \n",
        "\n",
        "                    SEED = 42\n",
        "                    random.seed(SEED)\n",
        "                    np.random.seed(SEED)\n",
        "                    tf.random.set_seed(SEED)"
      ],
      "metadata": {
        "id": "mOFTaZoqO1Y7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"center\">\n",
        "  <img src=\"https://user-images.githubusercontent.com/96804013/152835304-fa8af20e-d6c5-4b41-b36e-21b1ebe66240.png\")\n",
        "\"/>\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <em>Figure 3: The agents' interaction with the environment: </em>\n",
        "</p>\n",
        "\n",
        "\n",
        "Before we jump into the task, lets get familiar with OpenAI Gym terminologies and CartPole-v1 environment.\n",
        "\n",
        "###Terminologies\n",
        "---\n",
        "- Step = an agent taking one action in the environment\n",
        "- Observation = the agents' view of the environment state\n",
        "- Reward = a value assigned by the environment on how \"good\" the last action taken by the agent\n",
        "- Done = whether or not the current episode is terminated\n",
        "\n",
        "Run the snippet below to print shapes and values. \n",
        "\n",
        "                    random_action = env.action_space.sample()\n",
        "                    env.reset()\n",
        "                    observation, reward, done, info = env.step(random_action)\n",
        "\n",
        "                    print(f\"Action = {random_action}\")\n",
        "                    print(f\"Observation = {observation}, shape = {observation.shape}\")\n",
        "                    print(f\"Reward = {reward}\")\n",
        "                    print(f\"Done = {done}\")"
      ],
      "metadata": {
        "id": "7o_76NK9O1WZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will write 2 helper functions: \n",
        "\n",
        "__1. select_action()__\n",
        "\n",
        "- The agent has a choice between whether to use the action that it has learned is the best one to take (exploitation) or to take a random action instead (exploration) \n",
        "- We use the value of exploration rate (which in the main loop linearly decays from 50% exploration to 2.5%) to decide what action to take\n",
        "- Use the snippet below to implement action selection function\n",
        "\n",
        "                    def select_action(exploration_rate, observation, dqn_agent):\n",
        "                      rand_num = random.random() # Flip a coin\n",
        "                      if rand_num <= exploration_rate:\n",
        "                        action = 0 if exploration_rate/2 < 0.05 else 1  \n",
        "                      else: \n",
        "                        action = np.argmax(dqn_agent.predict(observation)[0])\n",
        "                       \n",
        "                      return action "
      ],
      "metadata": {
        "id": "8Mia5h-KO6wQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__2. replay()__\n",
        "\n",
        "  - One of the major contributions of the DQN algorithm is the concept of a  replay memory (train itself on the stored transitions from past).\n",
        "\n",
        "- For every experience sampled, a target Q-value is the sum of reward that we get after taking action `a` on state `s` and the discounted max `Q` value among all possible actions from next state `s'` \n",
        "\n",
        "<br>\n",
        "\n",
        "$$Q(s,a) = r(s,a) + \\gamma \\cdot max_{a} Q(s', a)~~..........~~(equation~1)$$\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "## Exercise 1\n",
        "\n",
        "- In the snippet below for replay function, fill in the code for to implement equation 1.\n",
        "\n",
        "- Hints: \n",
        "    - $r(s,a) = reward$\n",
        "    - $\\gamma$ is taken as an agrument (gamma) in the function\n",
        "    - For max operation use the np.amax() from numpy, reference ([link](https://numpy.org/doc/stable/reference/generated/numpy.amax.html))\n",
        "    - use `dqn_agent.predict(observation_next[0])` to use the get the Q-values\n",
        "\n",
        "                    def replay(memory, dqn_agent, batch_size, gamma):\n",
        "                      minibatch = random.sample(memory, batch_size)\n",
        "                      for current_observation,action, reward, observation_next, done in minibatch: \n",
        "                        if not done:\n",
        "                          target = *******************YOUR CODE HERE *******************\n",
        "\n",
        "                        else: \n",
        "                          target = reward \n",
        "\n",
        "                        current_q = dqn_agent.predict(current_observation) \n",
        "                        current_q[0][action] = target \n",
        "                        dqn_agent.fit(current_observation, current_q, epochs=1, verbose=0)"
      ],
      "metadata": {
        "id": "1aAip665O6tv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are going to set some learning parameters\n",
        "\n",
        "                    max_episodes = 100 \n",
        "                    max_timesteps = 200 \n",
        "                    gamma = 0.95 # Discount factor\n",
        "                    memory = deque(maxlen=800)\n",
        "\n",
        "                    num_actions = env.action_space.n\n",
        "                    print(f\"Number of actions that can be taken = {num_actions}\")\n",
        "                    num_observations = env.observation_space.shape[0]\n",
        "\n",
        "                    batch_size = 16 \n",
        "                    learning_rate = 0.001\n",
        "                    Adam = keras.optimizers.Adam "
      ],
      "metadata": {
        "id": "Zav3B590O6rJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The DQN agent is a neural network model that takes as inputs observations from the environment and output an action to take. \n",
        "\n",
        "---\n",
        "## Exercise 2\n",
        "\n",
        "\n",
        "Initialize a Sequential model with following code: \n",
        "\n",
        "                  dqn_agent = Sequential()\n",
        "\n",
        "Use `dqn_agent.add()` function to build a DQN agent with the following architecture.\n",
        "  - Dense layer with `24` output units and `relu` activation, remember to specity `input_dim`\n",
        "  - Dense layer with `24` output units and `relu` activation\n",
        "  - Dense layer with `num_actions` output units and `linear` activation\n",
        "\n",
        "- Compile the `dqn_agent` with `mse` loss and `Adam` optimizer (make sure you specify the `learning_rate` here)\n",
        "\n",
        "- Print the `dqn_agent` model summary\n",
        "- References: \n",
        "  - Dense ([link](https://keras.io/api/layers/core_layers/dense/))"
      ],
      "metadata": {
        "id": "OGuI57c3POBU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Exercise 3\n",
        "\n",
        "In the code snippet below, please fill in the the empty spaces.\n",
        "\n",
        "Hints: \n",
        "\n",
        "1. __Select action:__ make use of the select_action function defined above\n",
        "2. __Take a step in the environment__ make use of the step() function\n",
        "3. __Gather experience:__ what constitutes a single experience?\n",
        "\n",
        "\n",
        "                    start = time.time()\n",
        "                    for episode in range(max_episodes): \n",
        "                      print(f\"Training episode {episode}\", end = '\\t')\n",
        "\n",
        "                      # Start each episode by initializing the environment (reset)\n",
        "                      current_observation = env.reset().reshape(1,-1) # Make shape compatible\n",
        "\n",
        "                      for timestep in range(max_timesteps): \n",
        "\n",
        "                        # Linearly decay from 80% exploration to 2.5%\n",
        "                        current_exploration_rate = 0.5*max(0.05, (max_episodes- 5*episode)/max_episodes)\n",
        "\n",
        "                        # select an action according to epsilon greedy strategy \n",
        "                        action = *******************YOUR CODE HERE(1)*******************\n",
        "                        \n",
        "                        # Take a step in the environment\n",
        "                        observation_next , reward, done, info  = *******************YOUR CODE HERE(2)*******************\n",
        "                        observation_next = observation_next.reshape(1,-1)\n",
        "\n",
        "                        # Gather the experience and append it o memory \n",
        "                        experience = (*******************YOUR CODE HERE(3)*******************)\n",
        "                        memory.append(experience)\n",
        "\n",
        "                        # If we have enough memories collected\n",
        "                        if len(memory) > batch_size: \n",
        "                          # Replay the memory of experiences collected so far (to train agent)\n",
        "                          replay(memory, dqn_agent, batch_size, gamma)\n",
        "\n",
        "                        # We are done with this timestep, so current observation <-- next observation\n",
        "                        current_observation = observation_next\n",
        "\n",
        "                        if done: \n",
        "                          # If the agent fails, episodes are terminated before max_timesteps\n",
        "                          print(f\"completed with {timestep + 1} timesteps\", end = '\\n')\n",
        "                          break # Get out of the loop to next episode if this episode is finished \n",
        "                      \n",
        "                    # Save a trained agent (at the end of all episodes)\n",
        "                    dqn_agent.save(\"my_agent\")\n",
        "\n",
        "                    env.close()\n",
        "                    print(f\"Total training time taken = {time.time() - start} seconds\")\n",
        "                    trained_agent = tf.keras.models.load_model(\"my_agent\")"
      ],
      "metadata": {
        "id": "ts_kuXIePN_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "- The increase in the number of timesteps as the episodes progress is a good indicator for the agent learning to perform its task successfully. \n",
        "\n",
        "- Since fully training the agent can take a long time, You have an option to __terminate the training cell__ above and load an agent that was trained for 500 episodes and see the performance.\n",
        "\n",
        "- To do so, use the following code: \n",
        "\n",
        "                    !wget -o -q https://github.com/poudel-bibek/Intro-to-AI-Assignments/files/8027697/my_agent.zip\n",
        "                    !unzip -o -q ./my_agent.zip -d unzipped/ \n",
        "                    trained_agent = tf.keras.models.load_model('./unzipped/my_agent')"
      ],
      "metadata": {
        "id": "WCMM0vkcPN8n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Now, use the code below (either with the hosted agent or the agent that you trained) to render a rollout of agent performing the cartpole balancing task.\n",
        "\n",
        "\n",
        "                    !pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "                    !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "                    !pip install colabgymrender==1.0.2\n",
        "\n",
        "---\n",
        "\n",
        "                    from colabgymrender.recorder import Recorder \n",
        "\n",
        "                    env = gym.make(\"CartPole-v1\")\n",
        "                    env = Recorder(env, './video')\n",
        "                    observation = env.reset().reshape(1,-1)\n",
        "\n",
        "                    for timestep in range(max_timesteps):\n",
        "                      learned_action = np.argmax(trained_agent.predict(observation)[0])\n",
        "                      observation, reward, done, info = env.step(learned_action)  \n",
        "                      observation = observation.reshape(1,-1)\n",
        "\n",
        "                      if done: \n",
        "                        break \n",
        "\n",
        "                    env.play()"
      ],
      "metadata": {
        "id": "8IwRQGl_Pa3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ux_FX0j1PNfS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}