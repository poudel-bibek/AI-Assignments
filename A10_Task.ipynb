{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A10 Task.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMLsr09wDcUfBPkVynCJDqZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poudel-bibek/Intro-to-AI-Assignments/blob/main/A10_Task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![bhi](https://user-images.githubusercontent.com/96804013/152689231-de4db6bd-e653-4dfc-8881-dc3feef3389d.png)\n"
      ],
      "metadata": {
        "id": "7uw5Bk7VO1bU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 10: Deep Q-Networks (Task)\n",
        "---\n",
        "\n",
        "In this assignment, we will use OpenAI Gym ([link](https://gym.openai.com/envs/CartPole-v1/)) to solve a classic control problem of Balancing a Cartpole. To do so, we will be training a Reinforcement Learning agent using the DQN algorithm ([Link](https://openai.com/blog/openai-baselines-dqn/)).\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://user-images.githubusercontent.com/96804013/152690560-12ee45f2-69bb-422c-b230-90e4430f15b3.gif\")\n",
        "\"/>\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <em>Figure 1: Performance of an untrained agent on Cartpole-v1</em>\n",
        "</p>\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://user-images.githubusercontent.com/96804013/152689694-f72560fd-cbc1-4f55-87fc-c5e21655d667.gif\")\n",
        "\"/>\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <em>Figure 2: Performance of agent during training</em>\n",
        "</p>\n",
        "\n",
        "Let's start by importing necessary packages, libraries and specifying the environment. \n",
        "\n",
        "                    import time\n",
        "                    import gym\n",
        "                    import random\n",
        "                    import numpy as np\n",
        "                    from collections import deque\n",
        "\n",
        "                    import tensorflow as tf\n",
        "                    from tensorflow import keras\n",
        "                    from keras.models import Sequential\n",
        "                    from keras.layers import Dense\n",
        "\n",
        "                    env = gym.make(\"CartPole-v1\")\n"
      ],
      "metadata": {
        "id": "mOFTaZoqO1Y7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"center\">\n",
        "  <img src=\"https://user-images.githubusercontent.com/96804013/152835304-fa8af20e-d6c5-4b41-b36e-21b1ebe66240.png\")\n",
        "\"/>\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <em>Figure 3: The agents' interaction with the environment: </em>\n",
        "</p>\n",
        "\n",
        "\n",
        "Before we jump into the task, lets get familiar with OpenAI Gym terminologies and CartPole-v1 environment.\n",
        "\n",
        "###In the CartPole-v1 environment: \n",
        "---\n",
        "__Actions:__\n",
        "  - 0 = Push cart to the left\n",
        "  - 1 =\tPush cart to the right \n",
        "\n",
        "__Observations:__\n",
        "  - Cart Position \n",
        "  - Cart Velocity \n",
        "  - Pole Angle \n",
        "  - Pole Velocity At Tip\n",
        "\n",
        "__Reward:__ For each timestep that the agent is able to NOT FAIL, it collects a  reward of `+1`\n",
        "\n",
        "__Terminologies:__\n",
        "\n",
        "- Step = an agent taking one action in the environment\n",
        "- Observation = the agents' view of the environment state (for this assignment, terms `states` and `observations` are used equivalently.)\n",
        "- Reward = a value assigned by the environment on how \"good\" the last action taken by the agent\n",
        "- Done = whether or not the current episode is terminated\n",
        "\n",
        "Run the snippet below to print various exploratory values. \n",
        "\n",
        "                    random_action = env.action_space.sample()\n",
        "                    env.reset()\n",
        "                    observation, reward, done, info = env.step(random_action)\n",
        "\n",
        "                    print(f\"Action = {random_action}\")\n",
        "                    print(f\"Observation = {observation}, shape = {observation.shape}\")\n",
        "                    print(f\"Reward = {reward}\")\n",
        "                    print(f\"Done = {done}\")\n",
        "\n",
        "                    print(f\"Number of actions that can be taken = {env.action_space.n}\")\n",
        "                    print(f\"Limits of the observation: \\n max ={env.observation_space.low} \\n min ={env.observation_space.high}\")"
      ],
      "metadata": {
        "id": "7o_76NK9O1WZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Now, we will write 2 helper functions: \n",
        "\n",
        "__1. select_action()__\n",
        "\n",
        "- The agent has a choice between whether to use the action that it has learned is the best one to take (exploitation) or to take a random action instead (exploration) \n",
        "- We use the value of exploration rate (which in the main loop linearly decays from 50% exploration to 2.5%) to decide what action to take\n",
        "- Use the snippet below to implement action selection function\n",
        "\n",
        "                    def select_action(min_exploration_rate, current_exploration_rate, observation, dqn_agent):\n",
        "                      rand_num = random.random() \n",
        "                      if rand_num <= current_exploration_rate:\n",
        "                        action = 0 if current_exploration_rate <= min_exploration_rate/2 else 1 \n",
        "                      else: \n",
        "                        action = np.argmax(dqn_agent.predict(observation)[0])\n",
        "                      return action "
      ],
      "metadata": {
        "id": "8Mia5h-KO6wQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__2. replay()__\n",
        "\n",
        "  - The two major contributions of the DQN algorithm are:\n",
        "    - The use of a target network: there are 2 copies of the agent namely, policy network and target network. The q_values agent currently assigns for every state, action pair come from policy network whereas the aspirational q_values for every state, action pair (what the q_values should be) come from a target network. In the main training loop, the target network gets a copy of the policy network every N episodes (chosen as 10 here)\n",
        "\n",
        "    - The concept of a  replay memory: collect the transitions experiences of the agent into a replay memory, every episode sample a batch of (experience, target values) from the replay memory and train the agent on it.\n",
        "\n",
        "\n",
        "- For every experience sampled, a target Q-value is the sum of reward that we get after taking action `a` on state `s` and the discounted max `Q` value among all possible actions from next state `s'`.  The `Q` in the equation below is the target network. \n",
        "\n",
        "<br>\n",
        "\n",
        "$$Q(s,a) = r(s,a) + \\gamma \\cdot max_{a} Q(s', a)~~..........~~(equation~1)$$\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "---\n",
        "## Exercise 1\n",
        "\n",
        "- In the snippet below for replay function, fill in the code for to implement equation 1.\n",
        "\n",
        "- Hints: \n",
        "    - $r(s,a) = reward$\n",
        "    - $\\gamma$ is taken as an agrument (gamma) in the function\n",
        "    - For max operation use the np.amax() from numpy, reference ([link](https://numpy.org/doc/stable/reference/generated/numpy.amax.html))\n",
        "    - use `target_agent.predict(observation_next)[0]` to use the get the Q-values\n",
        "\n",
        "                    def replay(memory, dqn_agent, target_agent, batch_size, gamma):\n",
        "                      states_batch = [] \n",
        "                      q_values_batch = []\n",
        "\n",
        "                      minibatch = random.sample(memory, batch_size)\n",
        "                      for current_observation,action, reward, observation_next, done in minibatch: \n",
        "                        if not done:\n",
        "                          target = ############ YOUR CODE HERE ###############\n",
        "                        else: \n",
        "                          target = reward \n",
        "\n",
        "                        current_q = dqn_agent.predict(current_observation)\n",
        "                        current_q[0][action] = target \n",
        "\n",
        "                        states_batch.append(current_observation[0])\n",
        "                        q_values_batch.append(current_q[0]) \n",
        "\n",
        "                      states_batch = np.array(states_batch)\n",
        "                      q_values_batch = np.array(q_values_batch)\n",
        "                      dqn_agent.fit(states_batch, q_values_batch, epochs=1, verbose=0) \n",
        "                      return dqn_agent "
      ],
      "metadata": {
        "id": "1aAip665O6tv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are going to set some learning parameters\n",
        "                \n",
        "                    max_episodes = 75 \n",
        "                    gamma = 0.97 \n",
        "                    memory = deque(maxlen=256)\n",
        "                    target_update_steps = 10\n",
        "\n",
        "                    num_actions = env.action_space.n\n",
        "                    print(f\"Number of actions that can be taken = {num_actions}\")\n",
        "                    num_observations = env.observation_space.shape[0]\n",
        "\n",
        "\n",
        "                    batch_size = 64\n",
        "                    learning_rate = 1e-4 #0.0001\n",
        "                    Adam = keras.optimizers.Adam "
      ],
      "metadata": {
        "id": "Zav3B590O6rJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>\n",
        "The DQN agent is a 4-layered neural network model that takes as inputs observations from the environment and output an action to take. \n",
        "\n",
        "---\n",
        "## Exercise 2\n",
        "\n",
        "\n",
        "\n",
        "Initialize a Sequential model with following code: \n",
        "\n",
        "                  dqn_agent = Sequential()\n",
        "\n",
        "Use `dqn_agent.add()` function to build a DQN agent with the following architecture.\n",
        "  - Dense layer with `64` output units and `relu` activation, remember to specity `input_dim`\n",
        "  - Dense layer with `64` output units and `relu` activation\n",
        "  - Dense layer with `24` output units and `relu` activation\n",
        "  - Dense layer with `num_actions` output units and `linear` activation\n",
        "\n",
        "- Compile the `dqn_agent` with `mse` loss and `Adam` optimizer (make sure you specify the `learning_rate` here)\n",
        "\n",
        "- Print the `dqn_agent` model summary\n",
        "- References: \n",
        "  - Dense ([link](https://keras.io/api/layers/core_layers/dense/))\n",
        "  - Compile ([link](https://keras.io/api/models/model_training_apis/))"
      ],
      "metadata": {
        "id": "OGuI57c3POBU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br><br>\n",
        "\n",
        "Next, we will set some parameters that influence how \"exploratory\" our agent is.\n",
        "The exploration strategy that we will use is going to be exponentially decaying $\\epsilon$-greedy strategy given by: \n",
        "$$\\text{For episode n:}$$\n",
        "$$ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\text{current exploration probability(%)}~=max[~ϵ_{min}, ~ϵ_{start}(ϵ_{decay~rate})^n] $$\n",
        "\n",
        "We will start the agent to perform `100%` exploratory actions at the start. The probability then exponentially decreases (as a function of episode number) to a minimum exploration of `5%`.\n",
        "\n",
        "Set params using: \n",
        "\n",
        "                  eps_start = 1.0\n",
        "                  eps_decay_rate = 0.95\n",
        "                  eps_min = 0.05"
      ],
      "metadata": {
        "id": "ts_kuXIePN_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Exercise 3\n",
        "\n",
        "__Main training loop:__\n",
        "\n",
        "In the code snippet below, please fill in the the empty spaces.\n",
        "\n",
        "Hints: \n",
        "\n",
        "1. __Select action:__ make use of the select_action function defined above\n",
        "2. __Take a step in the environment__ make use of the step() function\n",
        "3. __Gather experience:__ what constitutes a single experience?\n",
        "\n",
        "                    update_counter = 0 \n",
        "                    reward_collected = []\n",
        "                   \n",
        "                    target_agent = dqn_agent \n",
        "                    current_exploration_rate = eps_start \n",
        "                    \n",
        "                    start = time.time()\n",
        "                    for episode in range(max_episodes): \n",
        "\n",
        "                      print(f\"Episode {episode}:\", end = ' ')\n",
        "                      current_observation = env.reset().reshape(1,-1) \n",
        "\n",
        "                      done = False \n",
        "                      score = 0\n",
        "                      timestep = 0\n",
        "\n",
        "                      while not done:\n",
        "                        action = ##########YOUR CODE HERE (1) ##############\n",
        "                       \n",
        "                        observation_next , reward, done, info  = ##########YOUR CODE HERE (2) ##############\n",
        "                        observation_next = observation_next.reshape(1,-1)\n",
        "\n",
        "                        experience = (##########YOUR CODE HERE (3) ##############)\n",
        "                        memory.append(experience)\n",
        "\n",
        "                        current_observation = observation_next\n",
        "                        score += reward\n",
        "                        timestep+=1 \n",
        "\n",
        "                      # Out of while loop\n",
        "                      print(f\"Exploration rate = {round(current_exploration_rate,3)},\", end = \"\\t\")\n",
        "                      current_exploration_rate = max(eps_min, current_exploration_rate*eps_decay_rate) \n",
        "                      \n",
        "                      print(f\"completed with {timestep} timesteps, score = {score}\", end = '\\n')\n",
        "                      reward_collected.append(score)\n",
        "\n",
        "                      if len(memory) >= batch_size: \n",
        "                        dqn_agent  = replay(memory, dqn_agent, target_agent, batch_size, gamma)\n",
        "\n",
        "                      update_counter+= 1\n",
        "                      if update_counter% target_update_steps ==0: \n",
        "                        target_agent.set_weights(dqn_agent.get_weights()) \n",
        "\n",
        "                    env.close()\n",
        "                    print(f\"Total training time taken = {time.time() - start} seconds\")"
      ],
      "metadata": {
        "id": "WCMM0vkcPN8n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, use the snippet below to plot the reward collected during training: \n",
        "\n",
        "                    fig, ax = plt.subplots(figsize = (8,5))\n",
        "                    x = len(reward_collected)\n",
        "                    ax.plot(range(x), reward_collected)\n",
        "                    ax.set_title(f\"Reward Collected over {x} episodes\")\n",
        "                    ax.set_xlabel(\"Episodes\")\n",
        "                    ax.set_ylabel(\"Reward per episode\")\n",
        "                    ax.set_xticks(range(0,x+1,10));\n",
        "                    \n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://user-images.githubusercontent.com/96804013/153685218-326ad9a1-e6f9-49a8-aab0-358e76bf236e.png\")\n",
        "\"/>\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <em>Figure 4: Reward plot (Your plot may look different) </em>\n",
        "</p>\n",
        "\n"
      ],
      "metadata": {
        "id": "8IwRQGl_Pa3h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The increase in the `reward collected per episode` as the episodes progress is a good indicator for the agent learning to perform its task successfully. \n",
        "\n",
        "- Now, use the code below to render a rollout of agent performing the cartpole balancing task.\n",
        "\n",
        "\n",
        "                    !pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "                    !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "                    !pip install colabgymrender==1.0.2\n",
        "\n",
        "---\n",
        "\n",
        "                    from colabgymrender.recorder import Recorder \n",
        "\n",
        "                    env = gym.make(\"CartPole-v1\")\n",
        "                    env = Recorder(env, './video')\n",
        "\n",
        "                    observation = env.reset().reshape(1,-1)\n",
        "                    done = False\n",
        "                    while not done:\n",
        "                      learned_action = np.argmax(dqn_agent.predict(observation)[0])\n",
        "                      observation, reward, done, info = env.step(learned_action)  \n",
        "                      observation = observation.reshape(1,-1)\n",
        "                    env.play()"
      ],
      "metadata": {
        "id": "drCtdKj_6_Fc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional: \n",
        "\n",
        "- Since fully training the agent can take a long time, You have an option to __terminate the training cell__ above and load an agent that was trained for 1000 episodes and see the performance.\n",
        "\n",
        "- To do so, use the following code: \n",
        "\n",
        "                    !wget -o -q https://github.com/poudel-bibek/Intro-to-AI-Assignments/files/8027697/my_agent.zip\n",
        "                    !unzip -o -q ./my_agent.zip -d unzipped/ \n",
        "                    trained_agent = tf.keras.models.load_model('./unzipped/my_agent_50')\n",
        "---\n",
        "\n",
        "                    env = gym.make(\"CartPole-v1\")\n",
        "                    env = Recorder(env, './video')\n",
        "\n",
        "                    observation = env.reset().reshape(1,-1)\n",
        "                    done = False\n",
        "                    while not done:\n",
        "                      # Get best action from agent \n",
        "                      learned_action = np.argmax(trained_agent.predict(observation)[0])\n",
        "\n",
        "                      # Apply the best action in the environment\n",
        "                      observation, reward, done, info = env.step(learned_action)  \n",
        "                      observation = observation.reshape(1,-1)\n",
        "                    env.play()"
      ],
      "metadata": {
        "id": "Z6Y5nm2t8Tcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CzthEP6f8T1N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}