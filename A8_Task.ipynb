{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A8 Task.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMGNoPLv++g4U0ZK3O/HzF2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poudel-bibek/Intro-to-AI-Assignments/blob/main/A8_Task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![A8](https://user-images.githubusercontent.com/96804013/153293943-c89fabd1-8c6d-471c-ac21-f3d40cf1fb3c.png)\n"
      ],
      "metadata": {
        "id": "gdTEc1NOOaMt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 8: Bandits (Task)\n",
        "---\n",
        "\n",
        "In this assignment, we will be solving the clssic Reinforcement Learning problem known as multi-armed bandits ([link](https://en.wikipedia.org/wiki/Multi-armed_bandit)).\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://user-images.githubusercontent.com/96804013/153735094-7c469a0b-2c05-4489-a436-eefb8128a379.png\")\n",
        "\"/>\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <em>Figure 1: The Multi-armed bandit problem</em>\n",
        "</p>\n",
        "\n",
        "\n",
        "__The problem:__ \n",
        "  - Imagine that you are in a casino with 4 different slot machines each known as `\"one-armed bandit\"` (as they're known for robbing people). Each bandit consists of a lever/ arm that can be pulled to receive a payout. \n",
        "  - Some slot machines payout more frequently than others i.e., each bandit has its own probability of success which remains unkown to us.\n",
        "  - Our is to walk out of the casino with the most money. i.e., pull the bandit arm one by one to maximize the total reward collected.\n",
        "\n",
        "\n",
        "<!-- __Solution:__\n",
        "  - We will train an agent which we allow to choose actions, and each action has a reward that is returned according to a given, underlying probability distribution. -->\n",
        "\n",
        "Start by importing the necessary packages: \n",
        "\n",
        "                    import os\n",
        "                    import time\n",
        "                    import random\n",
        "                    import numpy as np\n",
        "                    import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "N6oYjhRiOaKY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our problem formulation is episodic, i.e., we will conduct many episodes each with multiple timesteps.\n",
        "\n",
        "At each timestep,\n",
        "\n",
        "- We take an action on one slot machine i.e., pick which bandit arm to pull. The version of Multi-armed bandits that we implement is known as `Bernoulli Bandits`, based on the idea of `Bernoulli experiments`, where each bandit behaves like a random variable with binary outcomes i.e., every time we pull a bandit arm, it can have exactly two outcomes `\"success\"` or `\"failure\"`(corresponding to the reward of either a 0 or a 1). \n",
        "- Further, the probability of success remains the same every time the experiment is conducted. \n",
        "\n",
        "\n",
        "Lets use the following code to implement the reward behavior on a single timestep: \n",
        "\n",
        "                  def timestep(probabilities, arm_to_pull):\n",
        "                    rand_num = random.random()\n",
        "                    if rand_num <= probabilities[arm_to_pull]:\n",
        "                      reward = 1\n",
        "                    else: \n",
        "                      reward = 0\n",
        "                    return reward "
      ],
      "metadata": {
        "id": "9MUhoLXiOaHw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that, in the timestep function above, to provide a reward on each timestep, the function has access to the `\"true\"` payout probabilities. \n",
        "\n",
        "---\n",
        "\n",
        "Next, we will start by defininig an `Agent` that:\n",
        "\n",
        "- Keeps a track of:\n",
        "  1. Actions taken over an episode i.e, number of times each bandit arm was pulled over an episode. Remember that at each timestep, only a single arm can be pulled.\n",
        "  \n",
        "  2. `\"Quality\"` values of each action taken that estimates the expected reward that we can get on that action. A quality value indicates how good is it to take that particular action. At the start of an episode, quality values are initialized to zero and every timestep the quality values are updated.\n",
        "\n",
        "- Further, the agent has the capabilities to:\n",
        "  1. Take an action: the agent can choose which bandit arm to pull each timestep. To choose which arm to pull, it will use an `ϵ-greedy` (Epsilon greedy) action-selection strategy i.e., we balance exploration and exploitation such that the arm with highest known payout (based on maximum quality value) is chosen `1-ϵ` fraction of times and a randomly chosen arm is pulled `ϵ` fraction of times.\n",
        " \n",
        "  2. Update quality value: The quality value of an action is updated using the reward received after taking that action and the fraction of times that particular action has been taken this episode so far. At each timestep, only a single quality value (of one action) is updated using the update rule given by:\n",
        "\n",
        "  $$Quality~(a) ←  Quality~(a) + \\frac{1.0}{\\mathcal{N}_a}\\cdot[Reward(a) - Quality~(a)],$$\n",
        "\n",
        "where, $\\mathcal{N}_a = $ Number of times this action has been taken so far in this episode. Note that, the agent has no idea what the `\"true\"` payout probability of each bandit arm is.\n",
        "<br>\n",
        "\n",
        "<!-- Self-notes: Bibek: This has one problem, if all of them have equal Q-values, the probability of sampling is not equal. 0 is always chosen in this case because I use np.argmax() and for multiple indices with same Q-values, it always returns a 0. The correct implementaion should be randomly choose all indices with equal probabilities if they have same Q values. But this does not affect the results too much because such occurences are extremely rare (<1%) -->\n",
        "\n",
        "Use the following code to implement an agent: \n",
        "\n",
        "                    class Agent:\n",
        "                      def __init__(self, num_actions):\n",
        "                        self.num_actions = num_actions \n",
        "                        self.actions_track = np.zeros(num_actions, dtype=np.int) \n",
        "                        self.quality_values = np.zeros(num_actions, dtype=np.float)\n",
        "\n",
        "                      def update_quality_value(self, action, reward):\n",
        "                        self.actions_track[action] += 1\n",
        "                        self.quality_values[action] += (1.0/self.actions_track[action]) * (reward - self.quality_values[action])\n",
        "                        \n",
        "                      def get_action(self, eps):\n",
        "                        if np.random.random() < eps: # explore\n",
        "                          return np.random.randint(self.num_actions)\n",
        "                        else: \n",
        "                          return np.argmax(self.quality_values)\n"
      ],
      "metadata": {
        "id": "9F1TxpCtOaFb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An episode starts by initializing a new agent after which we go over a number of timesteps where,\n",
        "\n",
        "Each timestep in an episode is a `Bernoulli experiment` where: \n",
        "  - Ask the agent what action to take\n",
        "  - Take a timestep with that action and collect reward \n",
        "  - Update quality values for the action taken\n",
        "\n",
        "After the timestep ends, we collect actions and rewards over the episode \n",
        "\n",
        "---\n",
        "##Exercise 1\n",
        "\n",
        "__episode function:__\n",
        "- Takes as input the `true` payout probabilities (for reward calulation) and number of time steps in an episode. \n",
        "\n",
        "- Ouputs the actions taken in each timestep over an episode as well as the rewards collected from each actions over an episode.\n",
        "\n",
        "In the implementation of the episode function below, fill in your code:\n",
        "\n",
        "                    def episode(probs, steps_per_episode):\n",
        "                      agent = Agent(len(probs))\n",
        "                      actions_per_episode= [] \n",
        "                      rewards_per_episode = []\n",
        "\n",
        "                      for step in range(steps_per_episode):\n",
        "\n",
        "                        action = ######## YOUR CODE HERE (1) #########\n",
        "                        reward = ######## YOUR CODE HERE (2) #########\n",
        "                        \n",
        "                        agent.update_quality_value(action, reward)\n",
        "\n",
        "                        actions_per_episode.append(action)\n",
        "                        rewards_per_episode.append(reward)\n",
        "\n",
        "                      return np.array(actions_per_episode), np.array(rewards_per_episode)\n",
        "\n",
        "- Hints:\n",
        "    1. you can use `agent.get_action()` to get an action\n",
        "    2. reward is obtained each timestep by calling the `timestep` function defined above"
      ],
      "metadata": {
        "id": "er_u6jYhOaC4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, Use the following code to set some parameters and settings: \n",
        "\n",
        "                    probs = [0.10, 0.50, 0.60, 0.80] \n",
        "                    num_arms = len(probs)\n",
        "                    max_episodes = 10000 \n",
        "\n",
        "                    steps_per_episode = 500 \n",
        "                    eps = 0.1 ##### CHANGE THIS VALUE ######\n",
        "\n",
        "                    total_rewards = np.zeros((steps_per_episode,))  # reward history sum\n",
        "                    total_actions = np.zeros((steps_per_episode, num_arms))  # action history sum\n",
        "\n",
        "                    print(f\"Rewards to collect: {total_rewards.shape}\\nActions to collect: {total_actions.shape}\")\n",
        "\n",
        "\n",
        "- `total_rewards` keeps a track of rewards collected at each timestep over episodes.\n",
        "- `total_actions` keeps a track of actions taken at each timestep over episodes.\n",
        "\n",
        "Initialize `total_rewards` and `total_actions` keep a track of all rewards collected and actions taken each episode \n",
        "\n",
        "---\n",
        "##Exercise 2\n",
        "\n",
        "Re-run the entire Colab with with different values of `eps` below:\n",
        "Values should be between 0 and 1. \n",
        "\n",
        "- `eps` = 0 means the no exploration i.e., always take the best known action \n",
        "- `eps` = 1 means full exploration i.e., always take a random action\n",
        "\n",
        "What did you find?"
      ],
      "metadata": {
        "id": "fg7SimgqOaAe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Each episode is run independent of other i.e., there is no transfer of `knowledge` (things that the agent learns over an episode/ quality values) are not transferred from one episode to next.\n",
        "\n",
        "Now, use the code below to run all episodes and collect history of rewards and actions taken: \n",
        "\n",
        "                    for i in range(max_episodes):\n",
        "                      actions_episode, rewards_episode = episode(probs, steps_per_episode)  \n",
        "                      avg_reward_per_timestep = np.sum(rewards_episode) / steps_per_episode\n",
        "\n",
        "                      if i  % 100 == 0:\n",
        "                        print(f\"Episode {i}: average reward per timestep = {avg_reward_per_timestep}\")\n",
        "\n",
        "                      total_rewards += rewards_episode\n",
        "                      for j, a in enumerate(actions_episode):\n",
        "                        total_actions[j][a] += 1\n"
      ],
      "metadata": {
        "id": "gAEjNmlQOZ93"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After performing one `Bernoulli Experiment` for each timestep in each episode the agent gets estimation of the unkown probabilities. \n",
        "\n",
        "Plot 1: \n",
        "- Average reward each timestep \n",
        "\n",
        "Plot 2: \n",
        "- Across 10,000 experiments, the rate of arm selection per timestep\n",
        "- After 10,000 experiments, we are able to `\"estimate\"` the payout probabilities of each bandit.\n",
        "\n",
        "Why are the `x-axes` in the plots are timesteps?\n",
        " \n",
        " <p align=\"center\">\n",
        "  <img src=\"https://user-images.githubusercontent.com/96804013/153914850-08bf6212-e837-42e9-9336-e61a3ffa4185.png\")\n",
        "\"/>\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <em>Figure 2: Average reward per timestep</em>\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://user-images.githubusercontent.com/96804013/153914853-79a265f5-d176-4f05-b4a1-ec61414d7e6a.png\")\n",
        "\"/>\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <em>Figure 3: Probability estimation (The arm probabilities may be different in your case, try to find the eps that gets this probabilities)</em>\n",
        "</p>\n",
        "\n",
        "Use the following code to plot:\n",
        "\n",
        "                    # Plot reward results\n",
        "\n",
        "                    average_reward_per_timestep = total_rewards / max_episodes\n",
        "\n",
        "                    fig, ax = plt.subplots(figsize=(12, 5), dpi = 80)\n",
        "                    plt.grid()\n",
        "                    ax.plot(average_reward_per_timestep, '.')\n",
        "                    ax.set_xlabel(\"timestep\", fontsize=16)\n",
        "                    ax.set_ylabel(\"Average Reward per timestep\", fontsize=16)\n",
        "                    ax.set_ylim([0,1])\n",
        "                    ax.set_xlim([1, steps_per_episode])\n",
        "\n",
        "                    # Plot action results\n",
        "\n",
        "                    fig, ax = plt.subplots(figsize=(12, 5), dpi = 80)\n",
        "                    plt.grid()\n",
        "                    for i in range(num_arms):\n",
        "                      arm_pull_average = 100 * total_actions[:,i] / max_episodes\n",
        "                      steps = list(np.array(range(steps_per_episode))+1)\n",
        "                      ax.plot(steps, arm_pull_average, '-', linewidth = 2, label =f\"Arm{i}: {arm_pull_average[-1]}%\")\n",
        "\n",
        "                    ax.legend(fontsize=14)\n",
        "                    ax.set_xlabel(\"timestep\", fontsize=16)\n",
        "                    ax.set_ylabel(\"Average (%) arm choice over timesteps\", fontsize=16)\n",
        "                    ax.set_xlim([0, steps_per_episode])\n",
        "                    ax.set_ylim([0, 100])\n",
        "\n"
      ],
      "metadata": {
        "id": "UZPUkqJ_OZ7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4om3wBpEd0pE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}