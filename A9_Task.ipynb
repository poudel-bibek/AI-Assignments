{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A9 Task.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMM26rQf74pAxlHPTctPKDe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poudel-bibek/Intro-to-AI-Assignments/blob/main/A9_Task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Artboard](https://user-images.githubusercontent.com/96804013/153448807-fb099682-bac7-4254-986c-1d54ce6e534d.png)\n"
      ],
      "metadata": {
        "id": "d-SAeSplgQ5k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 9: Q-Learning (Task)\n",
        "---\n",
        "In this assignment, we will use OpenAI Gym ([link](https://gym.openai.com/envs/MountainCar-v0/)) to solve a classic control problem of driving a car uphill by rocking back and forth to gain momentum. To do so, we will be training a Reinforcement Learning agent using the Q-Learning algorithm ([Link](https://en.wikipedia.org/wiki/Q-learning)).\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://user-images.githubusercontent.com/96804013/153467131-a630edb2-aeb9-4694-bd69-28382b49c25a.gif\")\n",
        "\"/>\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <em>Figure 1: Performance of an untrained agent on MountainCar-v0</em>\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://user-images.githubusercontent.com/96804013/153467239-0e681df2-0f79-4453-81e0-b2ac1e9fd378.gif\")\n",
        "\"/>\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <em>Figure 2: Performance of a trained agent on MountainCar-v0</em>\n",
        "</p>\n",
        " \n",
        "\n",
        "Let's start by importing necessary packages, libraries and specifying the environment. \n",
        "                    \n",
        "                    import time\n",
        "                    import gym \n",
        "                    import random\n",
        "                    import numpy as np \n",
        "                    import matplotlib.pyplot as plt \n",
        "                    env = gym.make(\"MountainCar-v0\")"
      ],
      "metadata": {
        "id": "vRGh4OMygQ3v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"center\">\n",
        "  <img src=\"https://user-images.githubusercontent.com/96804013/152835304-fa8af20e-d6c5-4b41-b36e-21b1ebe66240.png\")\n",
        "\"/>\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <em>Figure 3: The agents' interaction with the environment: </em>\n",
        "</p>\n",
        "\n",
        "\n",
        "Before we jump into the task, lets get familiar with OpenAI Gym terminologies and MountainCar-v0 environment.\n",
        "\n",
        "###In the MountainCar-v0 environment: \n",
        "---\n",
        "__Actions:__ \n",
        "  - 0 =\tpush left\n",
        "  - 1 =\tno push\n",
        "  - 2 =\tpush right\n",
        "\n",
        "__Observations:__ \n",
        "  - Position\n",
        "  - Velocity\n",
        "\n",
        "__Reward:__ For each timestep that the agent spends to perform a task it collects a reward of `-1`. i.e., the goal is to perform the task as quickly as possible (with least -ve rewards).\n",
        "  \n",
        "__Terminologies:__\n",
        "\n",
        "- Step = an agent taking one action in the environment\n",
        "- Observation = the agents' view of the environment state (for this assignment, terms `states` and `observations` are used equivalently.)\n",
        "- Reward = a value assigned by the environment on how \"good\" the last action taken by the agent\n",
        "- Done = whether or not the current episode is terminated\n",
        "\n",
        "Run the snippet below to print various exploratory values. \n",
        "\n",
        "                    random_action = env.action_space.sample() \n",
        "                    env.reset() \n",
        "                    observation, reward, done, info = env.step(random_action)\n",
        "\n",
        "                    print(f\"Action = {random_action}\")\n",
        "                    print(f\"Observation = {observation}, shape = {observation.shape}\")\n",
        "                    print(f\"Reward = {reward}\")\n",
        "                    print(f\"Done = {done}\")\n",
        "\n",
        "                    print(f\"Number of actions that can be taken = {env.action_space.n}\")\n",
        "                    print(f\"Limits of the observation: \\n max ={env.observation_space.low} \\n min = {env.observation_space.high}\")"
      ],
      "metadata": {
        "id": "l9eppLRggQ1X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visually, our  Q-table looks like:\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://user-images.githubusercontent.com/96804013/153719045-4da1d0a1-25ba-4b69-94e9-b0ee67a288fa.jpg\")\n",
        "\"/>\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <em>Figure 4: Q-table for MountainCarV-0 with 20 discrete position and velocity spaces. (Q-values randomly populated)</em>\n",
        "</p>"
      ],
      "metadata": {
        "id": "6-3KzQdDgQzC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will implement 2 helper functions: \n",
        "\n",
        "__1. make_discrete:__ \n",
        "  - The observation (position, velocity) values that we get from the environment are continuous (the the ranges you explored above). Since we set up our Q-table in such a way that there can be a total of 20 possible position and velocity values each, we will convert the continuous values to fall into one of the possible dicrete values. \n",
        "\n",
        "__2. get_greedy_action:__\n",
        "  - Make use of the Q-table learned so far to get the best possible action to take given an observation.\n",
        "\n",
        "Use the code below to implement them: \n",
        "\n",
        "                    def make_discrete(observation):\n",
        "                      pos, vel =  observation\n",
        "                      pos_bin = int(np.digitize(pos, pos_space))\n",
        "                      vel_bin = int(np.digitize(vel, vel_space))\n",
        "                      return (pos_bin, vel_bin)\n",
        "\n",
        "                    def get_greedy_action(q_table, observation, actions=[0, 1, 2]):\n",
        "                      # For the current observation, get the most favorable action\n",
        "                      values = np.array([q_table[observation,a] for a in actions])\n",
        "                      greedy_action = np.argmax(values)\n",
        "                      return greedy_action\n"
      ],
      "metadata": {
        "id": "rNYMbxPggQwu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "##Exercise 1\n",
        "\n",
        "__select_action function__:\n",
        "  - Apply exploration (i.e., with a certain probability, apply a random action instead of the best action that we learned from the Q-table). \n",
        "\n",
        "In the implementation of select_action function below, fill in with your code\n",
        "\n",
        "                    def select_action(current_exploration_rate, greedy_action):\n",
        "                      rand_num = ######## YOUR CODE HERE (1) #########\n",
        "\n",
        "                      if rand_num < current_exploration_rate:\n",
        "                        action = ######## YOUR CODE HERE (2) #########\n",
        "                      else: \n",
        "                        action = greedy_action\n",
        "\n",
        "                      return action \n",
        "\n",
        "- Hints: \n",
        "    1. Sample a random number between 0 and 1 (from all possible numbers between 0 and 1) with uniform probability\n",
        "    2. Randomly choose either of actions `0`, `1` or `2`, with equal probability.\n",
        "\n",
        "- References: \n",
        "  - random.random ([link](https://www.w3schools.com/python/ref_random_random.asp))\n",
        "  - random.choice ([link](https://www.w3schools.com/python/ref_random_choice.asp))"
      ],
      "metadata": {
        "id": "BQJBMt74gfLk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will set some parameters that influence how \"exploratory\" our agent is.\n",
        "The exploration strategy that we will use is going to be exponentially decaying $\\epsilon$-greedy strategy given by: \n",
        "$$\\text{For episode n:}$$\n",
        "$$ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\text{current exploration probability(%)}~=max[~ϵ_{min}, ~ϵ_{start}(ϵ_{decay~rate})^n] $$\n",
        "\n",
        "We will start the agent to perform `100%` exploratory actions at the start. The probability then exponentially decreases (as a function of episode number) to a minimum exploration of `5%`.\n",
        "\n",
        "Set exploration specific params using: \n",
        "\n",
        "                  eps_start = 1.0\n",
        "                  eps_decay_rate = 0.95\n",
        "                  eps_min = 0.05\n",
        "\n",
        "Futher, set environment and learning specific params using: \n",
        "\n",
        "                  env._max_episode_steps = 1000\n",
        "                  max_episodes = 50000\n",
        "                  learning_rate = 0.1\n",
        "                  gamma = 0.99"
      ],
      "metadata": {
        "id": "ZG_g6YNIgfJi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Below is the Q-Learning algorithm that we will be implementing in the main training loop. \n",
        "\n",
        "For futher reathing go to `Sutton and Barto 6.5` ([link](http://www.incompleteideas.net/book/RLbook2020.pdf#%5B%7B%22num%22%3A1800%2C%22gen%22%3A0%7D%2C%7B%22name%22%3A%22XYZ%22%7D%2C79.2%2C415.572%2Cnull%5D))\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://user-images.githubusercontent.com/96804013/153653226-b68e09a6-b8db-48f1-80cf-42ad7425dd76.png\")\n",
        "\"/>\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <em>Figure 5: Q-Learning algorithm</em>\n",
        "</p>\n",
        "\n",
        "---\n",
        "## Exercise 2: \n",
        "\n",
        "In the main training loop code below, implement the following equation from the algorithm above. \n",
        "\n",
        "\n",
        "$$Q(S,A) ← Q(S,A) + \\alpha[ R + \\gamma Q(S', a) - Q(S,A)]$$\n",
        "\n",
        "where `S` = current_obs, `A` = action, `S'` = obs_next`α`, `a` = action_next and `α` is named`learning_rate`, `γ` is named `gamma`.\n",
        "\n",
        "                    current_exploration_rate = eps_start\n",
        "                    reward_collected = []\n",
        "                    start = time.time()\n",
        "\n",
        "                    for i in range(max_episodes):\n",
        "                      timestep = 0 \n",
        "                      score = 0\n",
        "                      done = False\n",
        "                      current_obs = env.reset()\n",
        "                      current_obs = make_discrete(current_obs)\n",
        "                      \n",
        "                      while not done:\n",
        "                        greedy_action = get_greedy_action(q_table, current_obs)\n",
        "\n",
        "                        action = select_action (current_exploration_rate, greedy_action)\n",
        "                        obs_next, reward, done, info = env.step(action)\n",
        "                        obs_next = make_discrete(obs_next)\n",
        "\n",
        "                        action_next = get_greedy_action(q_table, obs_next)\n",
        "\n",
        "                        q_table[current_obs, action] = ########### YOUR CODE HERE ##########\n",
        "                        timestep += 1\n",
        "                        score += reward\n",
        "                        current_obs = obs_next\n",
        "                        \n",
        "                      if i % 100 == 0:\n",
        "                        print(f\"Episode {i}: Exploration rate = {round(current_exploration_rate,3)}, completed with {timestep} timesteps, score = {score}\")\n",
        "                        reward_collected.append(score)\n",
        "                      \n",
        "                      current_exploration_rate = max(eps_min, current_exploration_rate*eps_decay_rate)\n",
        "      \n",
        "                    print(f\"total training time = {time.time() - start} seconds\")\n",
        "\n"
      ],
      "metadata": {
        "id": "UBxVqGlogfHi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the following code to get Reward plot:\n",
        "\n",
        "                    fig, ax = plt.subplots(figsize = (10,5))\n",
        "                    x = len(reward_collected)\n",
        "                    ax.plot(range(x), reward_collected)\n",
        "                    ax.set_title(f\"Reward Collected over {x*100} episodes\")\n",
        "                    ax.set_xlabel(\"Episodes\")\n",
        "                    ax.set_ylabel(\"Reward per episode\")\n",
        "\n",
        "                    x_labels = [f\"{int(i/1000)}k\" for  i in range(0, 100*x + 1, 1000)]\n",
        "                    ax.set_xticklabels(x_labels);\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://user-images.githubusercontent.com/96804013/153732989-c888162b-e0b0-4053-853e-6aaed5c7a93f.png\")\n",
        "\"/>\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <em>Figure 6: Reward plot</em>\n",
        "</p>\n",
        "\n",
        "- From the plot above: the agent learns to complete the task in less that 200 timesteps."
      ],
      "metadata": {
        "id": "rkz-pXm9gncR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The increase in the `reward collected per episode` as the episodes progress is a good indicator for the agent learning to perform its task successfully. \n",
        "\n",
        "- Now, use the code below to render a rollout of agent performing the MountainCar task.\n",
        "\n",
        "\n",
        "                    !pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "                    !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "                    !pip install colabgymrender==1.0.2\n",
        "\n",
        "---\n",
        "\n",
        "                    from colabgymrender.recorder import Recorder \n",
        "\n",
        "                    env = gym.make(\"MountainCar-v0\")\n",
        "                    env = Recorder(env, './video')\n",
        "\n",
        "                    done = False \n",
        "                    current_observation = env.reset() \n",
        "                    \n",
        "                    while not done:\n",
        "                      action = select_action(current_exploration_rate, q_table, current_observation)\n",
        "                      observation_next, reward, done, info = env.step(action)\n",
        "                      current_observation = observation_next\n"
      ],
      "metadata": {
        "id": "JXOB26_sgq_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0bwYE-7zgoW7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}